---
title: "ETC 5521 Assignment 1 Q2"
author: "Arindom Baruah (32779267)"
format: 
  html:
    number-sections: true
    theme: journal
---


![](https://editor.analyticsvidhya.com/uploads/3951420200902_blog_-forecasting-with-time-series-models-using-python_pt2_website.png)


<div class="warning" style='background-color:#8bd69f; color: #000301; border-left: solid #044716 4px; border-radius: 4px; padding:0.7em;'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>What are we trying to study ?</b></p>
<p style='margin-left:1em;'>


<b> A small cafe in the city of Melbourne is interested in determining whether the daily earnings depend on the weather. They compiled data for a period over 2000-2001 to study this question. An initial data analysis for the dataset will be conducted in the following sections to prepare the dataset for further exploratory data 

</p>
<p style='margin-bottom:1em; margin-right:1em; text-align:right; font-family:Georgia'> </b> <i></i>
</p></span>
</div>



```{r library-load}
#| echo: false
#| message: false
#| warning: false
library(kableExtra)
library(tidyverse)
library(here)
library(visdat)
library(lubridate)
library(plotly)
```



# Data description

As a part of the initial data analysis, we need to describe the various variables present in the dataset. The variables are described in @tbl-metdata.

```{r}
#| echo: false
#| label: tbl-metdata
#| tbl-cap: "Metadata"

desc <- tibble(var=c("dt", "wday", "revenue", "expend", "precip", "mint", "maxt", "source"),
               description=c("Date", 
              "Day of the week",
              "Daily revenue in hundreds, 11=1100", 
              "Daily expenses in hundreds",
              "Precipitation in mm", 
              "Minimum temperature, Celsius",
              "Maximum temperature, Celsius",
              "Source of the weather data"))
kbl(desc, table.attr = 'data-quarto-disable-processing="true"') |> 
  kable_styling(full_width = FALSE) |>
  column_spec(1, width="2cm", border_right = T) |>
  column_spec(2, width="10cm")
```




```{r load-data}
load("cafe.rda")
```




1. __dt__: Date of observation
2. __wday__: The day of the week corresponding to the observation date.
3. __revenue__: Revenue generated for the observation date.
4. __expend__: Expenditures incurred for the observation date.
5. __precip__: Precipitation value on the observation date.
6. __mint__: Minimum temperature on the observation date.
7. __maxt__: Maximum temperature on the observation date.
8. __source__: Source of the weather data.

Let us get an understanding of how the data looks like by checking @tbl-head.

```{r table}
#| label: tbl-head
#| tbl-cap: "Initial observations of the dataset"

kbl(head(cafe), table.attr = 'data-quarto-disable-processing="true"') |> 
  kable_styling(full_width = FALSE) 

```

# Data screening

The second step in the initial data analysis would be to check for the datatypes associated with each variable. We can identify the datatypes through the glimpse function as shown below.

```{r}
glimpse(cafe)
```


Let us now try to visualise the datatypes using a visdat plot.

```{r}
#| fig-cap: Datatypes for each variable
#| label: fig-datviz

vis_dat(cafe)
```

Based on the glimpsed dataset and @fig-datviz, 

<div class="alert alert-block alert-warning">

⚠️‼️
We are able to observe the following issues with the datatypes of each variables:

1. <b> dt (Date) </b>: The date variable is in the character format and will need to be converted to datetime format.
2. <b> expend (Expenditure) </b>: The expenditure variable is in the character datatype while these are continuous numerical values. Hence, this variable will be converted to a numerical datatype.
3. <b> precip (Precipitation) </b>: The precipitation variable which shows the precipitation value is a character variable while it contains continuous numerical values. Hence, this variable will be converted to a numerical variable. 
</div>


Now that we have identified the variables with the incorrect datatypes, we will transform them into the required datatypes which will allow us to further analyse our dataset.

## Converting date to datetime

In order to convert the dates into suitable format, we will need to conduct a quick __sanity check__. Let us first check the dates in its current form.

```{r}
head(cafe$dt)
```
Based on the above output, we see that the dates are in the format of "MM-DD-YY". Hence, an appropriate treatment to the dates will be done to convert them into the required datetime format as shown through the code chunk below.

```{r}
cafe$dt <- mdy(cafe$dt)
```


## Obtaining day of week from date

While the column "wday" is already in its appropriate format and datatype, however, we will replace this column by obtaining the weekday from the date column. This would serve as an __added validation__ and will prevent any incorrect entries of weekday in the dataset. Furthermore, the column will be converted into an ordered factor datatype which would allow for easier analysis and visualisations in later sections.

```{r}
cafe$wday <- wday(cafe$dt,
                  label = TRUE)
```


## Converting expenditure and precipitation variables

The values of expenditures and precipitations should ideally be numerical in nature as they are continuous values. The following code chunk will convert the datatypes for these variables into a suitable datatype for numerical analysis.

```{r}
#| warning: false
cafe$precip <- as.numeric(cafe$precip)
cafe$expend <- as.numeric(cafe$expend)
```

Let us now visualise the new datatypes using the visdat package once again as illustrated by @fig-newdataviz

```{r}
#| label: fig-newdataviz
#| fig-cap: Datatypes of variables after data screening

vis_dat(cafe)
```

<div class="alert alert-block alert-success">
✅  Upon successful data screening, we now have a dataset with each variable being attributed with an appropriate datatype. ✅
</div>



# Data cleaning

In this section, we will be mainly concerned with the quality of our data. Upon assessing the quality of the data, we will utilise various data cleaning methods to improve the overall usability of the dataset.

## Missing values in the dataset

Let us try to visualise the presence of missing values in the dataset using the vis-miss visualisation.

```{r}
#| label: fig-vismis
#| fig-cap: Missing values in the dataset

vis_miss(cafe)
```


Based on illustration of @fig-vismis,

<div class="alert alert-block alert-warning">

⚠️‼️

While the overall dataset is fairly clean, the missing values constitute about <b> 0.4% </b> of the entire dataset. These missing values are observed in the expenditure,precipitation and the revenue variables. 

⚠️‼️

</div>

In the next part, we will observe how we can deal with the missing values in the dataset. Let us first visualise the distribution of the variables to get an understanding of a suitable method to either replace or remove the missing values in the dataset. This process is popularly termed as __data imputation__.


## Data imputation

### Precipitation


```{r}
#| label: fig-preciphist
#| fig-cap: Distribution of precipitation values
#| warning: false
#| message: false
pl1 <- ggplot(data = cafe, aes(x = precip)) +
  geom_histogram(fill = 'lightgreen', color = 'black') + geom_vline(xintercept = median(cafe$precip, na.rm = TRUE),
                                                                    color = 'red') + theme_classic() + labs(x = "Precipitation value (in mm)", y = "Number of observations") + annotate("text", x = 14, y = 85, label = paste0("Median precipitaion value:",median(cafe$precip,na.rm = TRUE)),color='red') 
pl1


```

@fig-preciphist illustrates the distribution of precipitation values. Upon analysing the data, we can observe that on most days, the precipitation values are low. The median value of precipitation amounted to `r median(cafe$precip,na.rm=TRUE)` mm. Furthermore, median values are generally unaffected by the presence of outlier values.

Hence, __replacing the missing values with the median values could be a safe assumption to make__. 

Let us imputate the  missing values through the code chunk below.

```{r}
cafe$precip <- cafe$precip %>% replace_na(median(cafe$precip,na.rm=TRUE))
```




### Expenditure

```{r}
#| label: fig-expendhist
#| fig-cap: Distribution of expenditure values
#| warning: false
#| message: false
pl2 <- ggplot(data = cafe, aes(x = expend)) +
  geom_histogram(fill = 'lightblue', color = 'black') + geom_vline(xintercept = median(cafe$expend, na.rm = TRUE),
                                                                    color = 'red') + theme_classic() + labs(x = "Expenditure (in hundreads) $", y = "Number of observations") + annotate("text", x = 2, y = 85, label = paste0("Median expenditure value:",median(cafe$expend,na.rm = TRUE)),color='red') 
pl2


```

@fig-expendhist illustrates a histogram of the distribution of the expenditure values. The median value of precipitation amounted to `r median(cafe$expend *100,na.rm=TRUE)` $.

Let us imputate the  missing expenditure values with the median values through the code chunk below.

```{r}
cafe$expend <- cafe$expend %>% replace_na(median(cafe$expend,na.rm=TRUE))
```


### Revenue


```{r}
#| label: fig-expendrev
#| fig-cap: Distribution of revenue values
#| message: false
#| warning: false
pl3 <- ggplot(data = cafe, aes(x = revenue)) +
  geom_histogram(fill = 'lightblue', color = 'black') + geom_vline(xintercept = median(cafe$revenue, na.rm = TRUE),
                                                                    color = 'red') + theme_classic() + labs(x = "Revenue (in hundreads) $", y = "Number of observations") + annotate("text", x = 22, y = 85, label = paste0("Median expenditure value:",median(cafe$revenue,na.rm = TRUE)),color='red') 
pl3


```

@fig-expendrev illustrates a histogram of the distribution of the revenue values. The median value of precipitation amounted to `r median(cafe$revenue *100,na.rm=TRUE)` $.

Let us imputate the  missing expenditure values with the median values through the code chunk below.

```{r}
cafe$revenue <- cafe$revenue %>% replace_na(median(cafe$revenue,na.rm=TRUE))
```


<div class="alert alert-block alert-success">
✅  We have successfully performed data imputation and removed all missing values by replacing them with median values of each variable. As a result, the data can be considered <b> clean </b>. ✅
</div>


# Conclusion

<div class="warning" style='background-color:#57bdd4; color: #000301; border-left: solid #044716 4px; border-radius: 4px; padding:0.7em;'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>What are our key takeaways ?</b></p>
<p style='margin-left:1em;'>


The following key steps were performed as a part of initial data analysis :

1. Datatypes for each variable were assessed and visualised.
2. Appropriate datatypes were assigned for date,expenditure and precipitation.
3. The weekday variable was obtained from the date of observation using the lubridate package to prevent presence of any incorrect data entries.
4. Data cleaning operations were conducted by checking for the presence of missing values.
5. Based on the distribution of each variable, data imputation was conducted to treat the missing values.

</p>
<p style='margin-bottom:1em; margin-right:1em; text-align:right; font-family:Georgia'> </b> <i></i>
</p></span>
</div>


# Resources

The above analysis was undertaken with the help of the following software and packages:

1. __RStudio__: Integrated Development for R. RStudio, PBC, Boston, MA URL http://www.rstudio.com/.
2. __ggplot2__: H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.
3. __tidyverse__: Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K,
  Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” _Journal of Open Source Software_,
  *4*(43), 1686. doi:10.21105/joss.01686 <https://doi.org/10.21105/joss.01686>.
4. __knitr__: Yihui Xie (2014) knitr: A Comprehensive Tool for Reproducible Research in R. In Victoria Stodden, Friedrich Leisch and Roger D. Peng, editors, Implementing
  Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595.
5. __rmarkdown__: Allaire J, Xie Y, Dervieux C, McPherson J, Luraschi J, Ushey K, Atkins A, Wickham H, Cheng J, Chang W, Iannone R (2023). _rmarkdown: Dynamic Documents for R_.
  R package version 2.23, <https://github.com/rstudio/rmarkdown>.
6. __kableExtra__: Zhu H (2023). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. http://haozhu233.github.io/kableExtra
